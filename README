ollama pull mistral
ollama run mistral

 ollama run llama3
streamlit run .\app.py

docker-compose down
d




✅ 1. Streamlit (sua aplicação RAG)
URL: http://localhost:8501

É aqui que você acessa a interface web para fazer perguntas com base nos documentos.

✅ 2. Ollama (servidor da LLM)
API: http://localhost:11434

curl http://localhost:11434/api/tags

curl http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{"model":"llama3.2", "prompt":"Jeaner é feliz?"}'
curl.exe http://localhost:11434/api/generate -H "Content-Type: application/json" -d "{\"model\":\"llama3.2\",\"prompt\":\"Jeaner é feliz?\"}"


curl http://localhost:11434/api/generate -d '{"model":"llama3.2","prompt":"Jeaner é feliz?"}'


Esse é o endpoint onde a aplicação se comunica com o modelo Mistral via HTTP.


Helm
winget install Helm.Helm
helm install ppa-chatbot ./charts/ppa-chatbot
helm upgrade --install ppa-chatbot ./charts/ppa-chatbot


pytest tests/
