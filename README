ollama pull mistral
ollama run mistral

 ollama run llama3.2
streamlit run .\app.py

 Após subir, carregar o modelo no Ollama (exemplo):
No terminal (dentro ou fora do container):

bash
Copiar
Editar
curl http://localhost:11434/api/pull -d '{"name": "mistral"}'
Ou configure para já carregar no CMD do serviço ollama.

❓ Quer que o Ollama já inicie com o modelo?
Podemos configurar assim no docker-compose.yml:

yaml
Copiar
Editar
command: ["ollama", "run", "mistral"]
Mas o ideal é deixar o container subir e você controlar os modelos conforme necessário.