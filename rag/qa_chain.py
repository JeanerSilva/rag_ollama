import os
import streamlit as st
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from rag.prompt import get_custom_prompt
from settings import TEMPERATURE

def build_qa_chain(vectorstore):
    if not vectorstore:
        st.warning("‚ö†Ô∏è Vetor de documentos n√£o carregado.")
        return None

    # L√™ o host do Ollama via vari√°vel de ambiente (com fallback)
    ollama_base_url = os.getenv("OLLAMA_HOST", "http://localhost:11434")

    # Inicializa a LLM
    try:
        llm = Ollama(
            model="llama3",              # üëà seu modelo principal
            base_url=ollama_base_url,
            temperature=TEMPERATURE
        )
    except Exception as e:
        st.error(f"‚ùå Erro ao conectar com o Ollama: {e}")
        return None

    # Define quantos trechos relevantes buscar (k)
    k = st.session_state.get("retriever_k", 6)

    # Cria a cadeia QA com recupera√ß√£o de fontes
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_type="similarity", k=k),
        return_source_documents=True,
        chain_type_kwargs={"prompt": get_custom_prompt()}
    )

    return qa_chain
